{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.65.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import *\n",
    "from dataset import *\n",
    "from helpfun import *\n",
    "import transforms as T\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据和设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=0)\n",
    "model =DeepLabV3_resnet50_stride16()\n",
    "# model =DeepLabV3_resnet50_stride8()\n",
    "# model =DeepLabV3_resnet34_stride16()\n",
    "# model =DeepLabV3_resnet18_stride16()\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "device=\"cuda\"\n",
    "lr=0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,weight_decay=1e-4)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "batch_size=20\n",
    "num_workers=21\n",
    "epoch=40\n",
    "max_acc=0\n",
    "\n",
    "\n",
    "name='DeepLabV3_resnet50_stride16' #实验名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "            T.Resize(224),\n",
    "            T.RandomHorizontalFlip(0.5),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "test_transforms = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "train_data = MyDataset(root=r'./data/', txt_file='./data/list/train_aug.txt'\n",
    "                        ,transforms=train_transforms)\n",
    "val_data = MyDataset(root=r'./data/', txt_file='./data/list/val.txt'\n",
    "                      ,transforms=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=val_data, batch_size=1,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/530 [00:17<15:23,  1.78s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------start training!-----------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mrange\u001b[39m(epoch)):\n\u001b[0;32m---> 24\u001b[0m     temp_acc,temp_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     train_acc\u001b[38;5;241m.\u001b[39mappend(temp_acc)\n\u001b[1;32m     26\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(temp_loss)\n",
      "File \u001b[0;32m~/work/VOC/helpfun.py:78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, net, optimizer, loss_fun, device, i, epoch, lr)\u001b[0m\n\u001b[1;32m     74\u001b[0m     train_correct \u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m (labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     76\u001b[0m     train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_correct\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# torch.save(net.state_dict(),  'net.pth')\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc=[]\n",
    "test_acc=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "logger = get_logger('%s.log' % (name))\n",
    "mk_model_dir(name)\n",
    "logger.info(\"-----------------------------------relate parameters!-----------------------------------\")\n",
    "logger.info(\"初始学习率：%.3f\"%(optimizer.param_groups[0]['lr']))#获取optimizer中的lr，保留3位小数，训练时打印（这时候的logger.info等同于print）并且记录在log中。\n",
    "logger.info(\"优化方式：%s\"%(str(optimizer).partition(' ')[0]))#打印optimizer输出为类名，类名中就会包含SGD、AdamW，通过字符串分割来获取。\n",
    "logger.info(\"最大学习次数：%.3f\"%(epoch))\n",
    "logger.info(\"batch_size：%.3f\"%(batch_size))\n",
    "logger.info(\"使用的网络：%s\"%(str(type(model)).partition(\"'\")[-1].partition('.')[-1][:-2]))#模型net不同于optimizer，打印net不输出类名，而是输出整个网络结构。使用type(net)获取类名，在使用字符串分割来获取。\n",
    "logger.info(summary(model, input_size=(batch_size, 3, 224, 224)))\n",
    "logger.info(model)\n",
    "logger.info(\"使用的损失函数：%s\"%(loss))#损失函数较为特殊，直接通过变量名criterion 得到CrossEntropyLoss()。\n",
    "\n",
    "logger.info(\"是否使用scheduler：%s\"%(scheduler if 'scheduler' in dir() else 'False'))\n",
    "logger.info(\"训练集使用的数据增强：\")\n",
    "logger.info('\\n'.join(map(str,print_transforms(train_transforms))))\n",
    "logger.info(\"测试集使用的数据增强：\")\n",
    "logger.info('\\n'.join(map(str,print_transforms(test_transforms))))\n",
    "logger.info(\"-----------------------------------start training!-----------------------------------\")\n",
    "for i in (range(epoch)):\n",
    "    temp_acc,temp_loss=train(train_loader, model, optimizer, loss,device,i,epoch,lr)\n",
    "    train_acc.append(temp_acc)\n",
    "    train_loss.append(temp_loss)\n",
    "\n",
    "    temp_acc,temp_loss=test(test_loader,loss, model,device)\n",
    "    test_acc.append(temp_acc)\n",
    "    test_loss.append(temp_loss)\n",
    "    if temp_acc>=max_acc:\n",
    "        max_acc=temp_acc\n",
    "        torch.save(model.state_dict(),  './model/{}/best.pth'.format(name))\n",
    "    print(\"Epoch: %d  Train Loss: %.3f, Acc: %.3f\"%(i+1,train_loss[i],train_acc[i]))\n",
    "    print(\"Epoch: %d  Test Loss: %.3f, Acc: %.3f\\n\"%(i+1,test_loss[i],test_acc[i]))\n",
    "    logger.info(\"Epoch: %d  Test Loss: %.3f, Acc: %.3f\\n\"%(i+1,test_loss[i],test_acc[i]))\n",
    "    torch.save(model.state_dict(), './model/{}/{}.pth'.format(name,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "96d32af43df8805957db0e5fc6e5fcf3acc7686f781ef48b23e746feb670d8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
